{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# File paths\n",
    "myanmar_file_path = './app/dataset/data_myn.txt'\n",
    "english_file_path = './app/dataset/data_eng.txt'\n",
    "\n",
    "# Read the files\n",
    "with open(myanmar_file_path, 'r', encoding='utf-8') as myfile:\n",
    "    myanmar_lines = myfile.readlines()\n",
    "\n",
    "with open(english_file_path, 'r', encoding='utf-8') as engfile:\n",
    "    english_lines = engfile.readlines()\n",
    "\n",
    "# Pair the lines\n",
    "paired_lines = list(zip(myanmar_lines, english_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        if txt_input is None:\n",
    "            return []\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for lang_data in batch:\n",
    "        for lang, tokens in lang_data.items():\n",
    "            processed_text = text_transform[lang](tokens) if tokens else torch.empty(0, dtype=torch.int64)\n",
    "            if lang == SRC_LANGUAGE:\n",
    "                src_batch.append(processed_text)\n",
    "                src_len_batch.append(processed_text.size(0))\n",
    "            elif lang == TRG_LANGUAGE:\n",
    "                trg_batch.append(processed_text)\n",
    "                \n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first = True) #<----need this because we use linear\n",
    "    # print('src_batch', src_batch.shape\n",
    "\n",
    "    # Set the length of empty sequences to the maximum length\n",
    "    max_len = max(src_batch.size(0), max(src_len_batch))\n",
    "    # print('max_length: ', max_len)\n",
    "    src_len_batch = [length if length > 0 else max_len for length in src_len_batch]\n",
    "    # print('src_len_batch', src_len_batch)\n",
    "\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first = True)\n",
    "\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('KaungHtetCho/Harry_Potter_LSTM') #load dataset from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('basic_english') # built-in tokenizer from torchtext\n",
    "\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])} \n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numericalization\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "vocab = build_vocab_from_iterator(tokenized_dataset['train']['tokens'], min_freq=3) \n",
    "vocab.insert_token('<unk>', 0)\n",
    "vocab.insert_token('<eos>', 1)\n",
    "vocab.set_default_index(vocab['<unk>']) # if there is no index, assigned '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size): # vocab = tokens to integer index\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for example in dataset:\n",
    "        if example['tokens']:\n",
    "            tokens = example['tokens'].append('<eos>')\n",
    "            tokens = [vocab[token] for token in example['tokens']]\n",
    "            data.extend(tokens) # creating new list # append = extra list\n",
    "\n",
    "    data = torch.LongTensor(data) # coverting to tensor int\n",
    "\n",
    "    num_batches = data.shape[0]  // batch_size # integer division # data.shape[0] = 12 as in example\n",
    "    data        = data[:num_batches * batch_size] \n",
    "\n",
    "    data = data.view(batch_size, num_batches) #view vs. reshape (whether data is contiguous)\n",
    "    return data #[batch size, seq len] \n",
    "\n",
    "# can use dataloader in pytorch way also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data  = get_data(tokenized_dataset['test'],  vocab, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
