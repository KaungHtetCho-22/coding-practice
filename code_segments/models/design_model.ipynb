{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3. detach hidden in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers # defining lstm (how many layers of LSTM)\n",
    "        self.hid_dim    = hid_dim    # vector size\n",
    "        self.emb_dim    = emb_dim    \n",
    "        \n",
    "        self.embedding  = nn.Embedding(vocab_size, emb_dim) # input the text > get_embedded > sent to LSTM (vectorized)\n",
    "                                                            # word -> embedding(vectorized)\n",
    "        \n",
    "        self.lstm       = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, dropout=dropout_rate, batch_first=True) # dropout connect -> drop weights between LSTM\n",
    "                                                                                                                   # in paper\n",
    "        \n",
    "        # seq length -> \n",
    "\n",
    "        self.dropout    = nn.Dropout(dropout_rate) # after certain process\n",
    "\n",
    "        # hidden dim to vocab size to softmax \n",
    "        self.fc         = nn.Linear(hid_dim, vocab_size) # prediction head\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    # optionally # inital weight with range\n",
    "    def init_weights(self):\n",
    "        # from the paper\n",
    "        # by bounding them into specific range, the weight doesn't go too big\n",
    "        \n",
    "        init_range_emb   = 0.1 \n",
    "        init_range_other = 1/math.sqrt(self.hid_dim) \n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_other) \n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_() # bias is not effecting a lot, then zero\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.emb_dim, self.hid_dim).uniform_(-init_range_other, init_range_other) #We #work with x\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hid_dim, self.hid_dim).uniform_(-init_range_other, init_range_other) #Wh #work with previous h\n",
    "    \n",
    "    # will be called in training (hidden,cell)\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device) # to take fully control of hidden \n",
    "        cell   = torch.zeros(self.num_layers, batch_size, self.hid_dim).to(device) \n",
    "        return hidden, cell\n",
    "           \n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden \n",
    "        hidden = hidden.detach() #not to be used for gradient computation\n",
    "        cell   = cell.detach()\n",
    "        return hidden, cell\n",
    "        \n",
    "    def forward(self, src, hidden): \n",
    "        #src: [batch_size, seq len]\n",
    "        embedding  = self.dropout(self.embedding(src)) #harry potter is ... # can learn pattern # embedding dropout\n",
    "        #embedding: [batch-size, seq len, emb dim]\n",
    "        output, hidden = self.lstm(embedding, hidden) \n",
    "        #ouput: [batch size, seq len, hid dim] # \n",
    "        #hidden: [num_layers * direction, seq len, hid_dim]\n",
    "        output     = self.dropout(output) # variation dropout is similar to dropout \n",
    "\n",
    "        #\n",
    "        prediction = self.fc(output)\n",
    "        #prediction: [batch_size, seq_len, vocab_size]\n",
    "        return prediction, hidden # to carry foward hidden"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
